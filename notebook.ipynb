{
 "cells": [
  {
    "cell_type": "code",
    "execution_count": null,
    "id": "transformers-example",
    "metadata": {},
    "outputs": [],
    "source": [
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "# Define model checkpoint and device (using CPU in Binder)\n",
      "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
      "device = \"cpu\"\n",
      "\n",
      "# Load tokenizer and model\n",
      "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
      "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
      "\n",
      "# Prepare inputs and generate model output\n",
      "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
      "outputs = model.generate(inputs)\n",
      "\n",
      "# Print the generated output\n",
      "print(tokenizer.decode(outputs[0]))\n"
    ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
