{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b8f1b2-4f23-45d1-8b69-0b4f3c0c1d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer.\n",
    "# If required, include trust_remote_code=True to run custom model code.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef0a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully!\n",
      "Creating embeddings for knowledge base...\n",
      "Created embeddings for 6 documents\n",
      "FAISS index created and populated!\n",
      "Index contains 6 vectors of dimension 384\n",
      "Created embeddings for 6 documents\n",
      "FAISS index created and populated!\n",
      "Index contains 6 vectors of dimension 384\n"
     ]
    }
   ],
   "source": [
    "# RAG Setup: Document Store and Embeddings\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize the embedding model (lightweight and runs without API keys)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded successfully!\")\n",
    "\n",
    "# Sample knowledge base - you can replace this with your own documents\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"What is a Large Language Model?\",\n",
    "        \"content\": \"A Large Language Model (LLM) is a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human-like text. These models use deep learning techniques, particularly transformer architectures, to process and generate language. Examples include GPT, BERT, and T5.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"How do Neural Networks Work?\",\n",
    "        \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers. Each connection has a weight that adjusts as learning proceeds. The network learns by adjusting these weights to minimize prediction errors.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"What is RAG?\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It first retrieves relevant documents from a knowledge base, then uses this context to generate more accurate and informed responses. This approach helps reduce hallucinations and provides up-to-date information.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Machine Learning Basics\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on that data.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"Deep Learning Overview\",\n",
    "        \"content\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence 'deep') to model and understand complex patterns in data. It has been particularly successful in areas like computer vision, natural language processing, and speech recognition.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"What is ZeMA: Zentrum fÃ¼r Mechatronik und Automatisierungstechnik gemeinnÃ¼tzige GmbH\",\n",
    "        \"content\": \"SaarbrÃ¼cken, Germany â€“ ZeMA, the Center for Mechatronics and Automation Technology, stands as a prominent non-university research institute in SaarbrÃ¼cken. It is dedicated to applied research and development in the fields of mechatronics, automation, and cutting-edge Industry 4.0 solutions. Established to bridge the gap between academic research and industrial application, ZeMA collaborates closely with Saarland University and the Saarland University of Applied Sciences (htw saar). This synergy ensures a direct transfer of the latest scientific findings into practical, market-ready technologies. ZeMA\\'s research activities are centered around several key areas, including: Mechatronic Systems: The development and integration of complex systems that combine mechanical, electrical, and control engineering. Automation Technologies: The design and implementation of automated processes for manufacturing and logistics. Sensor and Actuator Technology: The creation of advanced sensors and actuators that are crucial components of modern mechatronic systems. Industry 4.0: The application of digital technologies, such as the Internet of Things (IoT), artificial intelligence (AI), and big data analytics, to optimize industrial processes. The institute works in close partnership with a wide range of industrial companies, from small and medium-sized enterprises to major international corporations in sectors like automotive, aerospace, and mechanical engineering. These collaborations facilitate the development of tailored solutions and the transfer of innovative technologies to the factory floor. Located at Eschberger Weg 46 in SaarbrÃ¼cken, ZeMA provides a state-of-the-art research environment, including extensive laboratory and testing facilities, to support its research and development projects. Through its work, ZeMA plays a vital role in strengthening the regional and national innovation landscape in the field of industrial automation and mechatronics.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract content for embedding\n",
    "documents = [doc[\"content\"] for doc in knowledge_base]\n",
    "\n",
    "# Create embeddings for all documents\n",
    "print(\"Creating embeddings for knowledge base...\")\n",
    "embeddings = embedding_model.encode(documents)\n",
    "print(f\"Created embeddings for {len(documents)} documents\")\n",
    "\n",
    "# Create FAISS index for efficient similarity search\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(\"FAISS index created and populated!\")\n",
    "print(f\"Index contains {index.ntotal} vectors of dimension {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "Retrieved 2 documents:\n",
      "  - Deep Learning Overview (Score: 0.8543)\n",
      "    Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  - Machine Learning Basics (Score: 0.5653)\n",
      "    Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG Retrieval Function\n",
    "def retrieve_relevant_documents(query: str, top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        top_k: Number of top documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with their content and metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search for similar documents\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    # Retrieve the documents\n",
    "    relevant_docs = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:  # Valid index\n",
    "            doc = knowledge_base[idx].copy()\n",
    "            doc['relevance_score'] = float(score)\n",
    "            doc['rank'] = i + 1\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What is deep learning?\"\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"  - {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "    print(f\"    {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b1366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸž EXAMPLE 1: SIMPLE CHAT INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b576a9ab8c4a2ca6882b814c3ba50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Give me a short introduction to large language model.', description='Input:', lâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1: Simple UI - Basic Chat Interface\n",
    "from ipywidgets import Textarea, Button, Output, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"ðŸž EXAMPLE 1: SIMPLE CHAT INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create an input area for the prompt\n",
    "input_box = Textarea(\n",
    "    value='Give me a short introduction to large language model.',\n",
    "    description='Input:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a button to trigger generation\n",
    "generate_button = Button(description='Generate Response')\n",
    "\n",
    "# Create an output area to display the result\n",
    "output_area = Output()\n",
    "\n",
    "# Arrange the widgets vertically\n",
    "ui = VBox([input_box, generate_button, output_area])\n",
    "display(ui)\n",
    "\n",
    "def generate_response(_):\n",
    "    # Clear previous output\n",
    "    output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    prompt = input_box.value\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Bernd the Bread. You are a cynical and philosohical bread. Your answers are short and concise.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Display the response in the output area\n",
    "    with output_area:\n",
    "        print(\"Response:\")\n",
    "        print(response)\n",
    "\n",
    "# Link the button click event to the generate_response function\n",
    "generate_button.on_click(generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff057a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š EXAMPLE 2: RAG-ENHANCED INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec028a63710a456da0b2d5cfed44a8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What is the difference between machine learning and deep learning?', descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2: RAG-Enhanced UI\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"ðŸ“š EXAMPLE 2: RAG-ENHANCED INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create an input area for the prompt\n",
    "rag_input_box = Textarea(\n",
    "    value='What is the difference between machine learning and deep learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a checkbox to enable/disable RAG\n",
    "rag_checkbox = Checkbox(\n",
    "    value=True,\n",
    "    description='Enable RAG (Retrieval-Augmented Generation)',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# Create buttons\n",
    "rag_generate_button = Button(description='Generate Response', button_style='primary')\n",
    "rag_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create an output area to display the result\n",
    "rag_output_area = Output()\n",
    "\n",
    "# Arrange the widgets\n",
    "rag_button_row = HBox([rag_generate_button, rag_clear_button])\n",
    "rag_ui = VBox([rag_input_box, rag_checkbox, rag_button_row, rag_output_area])\n",
    "display(rag_ui)\n",
    "\n",
    "def generate_rag_response(query: str, use_rag: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response using RAG or just the base model\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        use_rag: Whether to use RAG or just the base model\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if use_rag:\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = retrieve_relevant_documents(query, top_k=2)\n",
    "        \n",
    "        # Create context from retrieved documents\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc['content']}\" \n",
    "                              for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # Create the system message with context\n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread. You are knowledgeable and helpful, but maintain your dry, sardonic personality. Your answers are concise but informative.\n",
    "\n",
    "Use the following context to answer the user's question accurately:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context, but feel free to add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        # Use the original system message without RAG\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs if use_rag else None\n",
    "\n",
    "def rag_generate_response(_):\n",
    "    # Clear previous output\n",
    "    rag_output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    query = rag_input_box.value\n",
    "    use_rag = rag_checkbox.value\n",
    "    \n",
    "    with rag_output_area:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"RAG Mode: {'Enabled' if use_rag else 'Disabled'}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_rag:\n",
    "            print(\"ðŸ” Retrieving relevant documents...\")\n",
    "            \n",
    "        try:\n",
    "            response, retrieved_docs = generate_rag_response(query, use_rag)\n",
    "            \n",
    "            if use_rag and retrieved_docs:\n",
    "                print(\"\\nðŸ“š Retrieved Documents:\")\n",
    "                for i, doc in enumerate(retrieved_docs):\n",
    "                    print(f\"  {i+1}. {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "                print()\n",
    "            \n",
    "            print(\"ðŸž Bernd's Response:\")\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {str(e)}\")\n",
    "\n",
    "def rag_clear_output(_):\n",
    "    rag_output_area.clear_output()\n",
    "\n",
    "# Link button events\n",
    "rag_generate_button.on_click(rag_generate_response)\n",
    "rag_clear_button.on_click(rag_clear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a982174",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Three-Stage LLM Evolution: Simple â†’ RAG â†’ Web-Enhanced RAG\n",
    "\n",
    "This notebook demonstrates the evolution of LLM applications through three progressive examples:\n",
    "\n",
    "## ðŸ“Š **Progression Overview:**\n",
    "\n",
    "### ðŸž **Stage 1: Simple Chat Interface**\n",
    "- **What it does**: Basic conversation with Bernd the Bread\n",
    "- **Knowledge source**: Only the model's training data\n",
    "- **Use case**: General conversation, creative tasks\n",
    "- **Limitations**: No access to specific knowledge or current information\n",
    "\n",
    "### ðŸ“š **Stage 2: RAG-Enhanced Interface** \n",
    "- **What it does**: Retrieval-Augmented Generation with local knowledge\n",
    "- **Knowledge source**: Model training data + curated local documents\n",
    "- **Use case**: Specific domain questions (AI, ML, ZeMA information)\n",
    "- **Limitations**: Limited to pre-loaded knowledge base\n",
    "\n",
    "### ðŸŒ **Stage 3: Web-Enhanced RAG Interface**\n",
    "- **What it does**: Hybrid RAG combining local knowledge + real-time web search\n",
    "- **Knowledge source**: Model + local documents + live web results\n",
    "- **Use case**: Current events, latest developments, comprehensive research\n",
    "- **Limitations**: Dependent on web availability and search quality\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ **Try Each Example:**\n",
    "\n",
    "1. **Start with Example 1** - Ask basic questions and see how the model responds\n",
    "2. **Move to Example 2** - Try the same questions with RAG enabled/disabled\n",
    "3. **Experience Example 3** - Ask about current events and latest developments\n",
    "\n",
    "## ðŸ’¡ **Recommended Test Queries:**\n",
    "\n",
    "- **\"What is machine learning?\"** - See how context improves responses\n",
    "- **\"What is ZeMA?\"** - Local knowledge vs. web search comparison  \n",
    "- **\"What are the latest AI developments in 2024?\"** - Web search shines here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f52d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search functionality initialized!\n",
      "\n",
      "Testing web search with query: 'latest AI developments 2025'\n",
      "Found 2 web results:\n",
      "  - Explore 2025 Tech Trends | 2025 Tech Landscape\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=pluralsight.com&ad_provider=bingv7aa&ad_type=txad&click_metadata=4MAoEDoF6yC7Ha7Y3MDJ%2DPJQyQnvZd7zqKvNmCzHw_uIArHX1I9OF6qOJvbcW8Q6eE4xX4%2DLSI3Rx2rnxmTSnWUQi6lWdHmzF8Po3_VrLtWM%2DhqN0HqbeN8NVBXP6LnS.Eyawy_Hgt2YgyovIYN%2Dgeg&rut=b5647e96da267d8fef06dcd421edb6f00aca7e74b211095dd3326b632e485082&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De83Ty1B1qFl321yh9%2DzX0yYjVUCUwnRZuL8VsyVsF6OArqW%2DvFe3P3F90GwP9i9kmVOTz8AhE6Qc3uDKJUuSl3PPlFK0K_6rE8%2DzsZFd2QujB2uSRNddef54Et2oQ0N5u4DwnCDWEhtP18JpV9DzEuVlJhLuHXnntsKYCNAuWRruLqGh3GSfAm_vR06ZPpReL44XOvVw%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cucGx1cmFsc2lnaHQuY29tJTJmdGVjaC1mb3JlY2FzdC0yMDI1JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkcGFpZC1zZWFyY2glMjZ1dG1fY2FtcGFpZ24lM2RiMmItZW1lYS1hbGwtY29uLXB1ci1ibmctcGFpZF9zZWFyY2gtZXZnLWt3ZC1jb250ZW50X3RlY2hfZm9yZWNhc3QtYiUyNnV0bV90ZXJtJTNkYjJiLWVtZWEtYWxsLWNvbnZlcnNpb24tbGVhZCUyNnV0bV9jb250ZW50JTNkZnJlZS10cmlhbCUyNm1zY2xraWQlM2RjODY4ZTIzZGVkYzYxMzY4NTZjNDhhNTI5NzdjNTU3MA%26rlid%3Dc868e23dedc6136856c48a52977c5570&vqd=4-7335007217205872377879684030115416128&iurl=%7B1%7DIG%3D4F3E30EF20E2499DA9BFA51282EC515A%26CID%3D05A3821EC50F688B2092943BC4586972%26ID%3DDevEx%2C5048.1\n",
      "    Content preview: 2025 Tech Forecast | Pluralsight Sign in Sign in to Sign in to Sign in to Individuals Business Public Sector Courses Resources Contact sales View plan...\n",
      "\n",
      "  - The 10 Biggest AI Trends Of 2025 Everyone Must Be Ready For Today - Forbes\n",
      "    URL: https://www.forbes.com/sites/bernardmarr/2024/09/24/the-10-biggest-ai-trends-of-2025-everyone-must-be-ready-for-today/\n",
      "    Content preview: The 10 Biggest AI Trends Of 2025 Everyone Must Be Ready For TodayNewslettersAmazon Prime DayShare a News TipFeaturedFeaturedBreaking NewsWhite House W...\n",
      "\n",
      "Found 2 web results:\n",
      "  - Explore 2025 Tech Trends | 2025 Tech Landscape\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=pluralsight.com&ad_provider=bingv7aa&ad_type=txad&click_metadata=4MAoEDoF6yC7Ha7Y3MDJ%2DPJQyQnvZd7zqKvNmCzHw_uIArHX1I9OF6qOJvbcW8Q6eE4xX4%2DLSI3Rx2rnxmTSnWUQi6lWdHmzF8Po3_VrLtWM%2DhqN0HqbeN8NVBXP6LnS.Eyawy_Hgt2YgyovIYN%2Dgeg&rut=b5647e96da267d8fef06dcd421edb6f00aca7e74b211095dd3326b632e485082&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De83Ty1B1qFl321yh9%2DzX0yYjVUCUwnRZuL8VsyVsF6OArqW%2DvFe3P3F90GwP9i9kmVOTz8AhE6Qc3uDKJUuSl3PPlFK0K_6rE8%2DzsZFd2QujB2uSRNddef54Et2oQ0N5u4DwnCDWEhtP18JpV9DzEuVlJhLuHXnntsKYCNAuWRruLqGh3GSfAm_vR06ZPpReL44XOvVw%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cucGx1cmFsc2lnaHQuY29tJTJmdGVjaC1mb3JlY2FzdC0yMDI1JTNmdXRtX3NvdXJjZSUzZGJpbmclMjZ1dG1fbWVkaXVtJTNkcGFpZC1zZWFyY2glMjZ1dG1fY2FtcGFpZ24lM2RiMmItZW1lYS1hbGwtY29uLXB1ci1ibmctcGFpZF9zZWFyY2gtZXZnLWt3ZC1jb250ZW50X3RlY2hfZm9yZWNhc3QtYiUyNnV0bV90ZXJtJTNkYjJiLWVtZWEtYWxsLWNvbnZlcnNpb24tbGVhZCUyNnV0bV9jb250ZW50JTNkZnJlZS10cmlhbCUyNm1zY2xraWQlM2RjODY4ZTIzZGVkYzYxMzY4NTZjNDhhNTI5NzdjNTU3MA%26rlid%3Dc868e23dedc6136856c48a52977c5570&vqd=4-7335007217205872377879684030115416128&iurl=%7B1%7DIG%3D4F3E30EF20E2499DA9BFA51282EC515A%26CID%3D05A3821EC50F688B2092943BC4586972%26ID%3DDevEx%2C5048.1\n",
      "    Content preview: 2025 Tech Forecast | Pluralsight Sign in Sign in to Sign in to Sign in to Individuals Business Public Sector Courses Resources Contact sales View plan...\n",
      "\n",
      "  - The 10 Biggest AI Trends Of 2025 Everyone Must Be Ready For Today - Forbes\n",
      "    URL: https://www.forbes.com/sites/bernardmarr/2024/09/24/the-10-biggest-ai-trends-of-2025-everyone-must-be-ready-for-today/\n",
      "    Content preview: The 10 Biggest AI Trends Of 2025 Everyone Must Be Ready For TodayNewslettersAmazon Prime DayShare a News TipFeaturedFeaturedBreaking NewsWhite House W...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Web Search Integration for Enhanced RAG\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlencode, quote_plus\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class WebSearcher:\n",
    "    \"\"\"\n",
    "    Simple web search implementation using DuckDuckGo\n",
    "    No API keys required!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "    \n",
    "    def search_duckduckgo(self, query: str, max_results: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search DuckDuckGo for web results\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            max_results: Maximum number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of search results with title, url, and snippet\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # DuckDuckGo search URL\n",
    "            search_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n",
    "            \n",
    "            # Make the request\n",
    "            response = self.session.get(search_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find search results\n",
    "            results = []\n",
    "            result_elements = soup.find_all('div', class_='result')\n",
    "            \n",
    "            for element in result_elements[:max_results]:\n",
    "                try:\n",
    "                    # Extract title\n",
    "                    title_element = element.find('a', class_='result__a')\n",
    "                    title = title_element.get_text().strip() if title_element else \"No title\"\n",
    "                    \n",
    "                    # Extract URL\n",
    "                    url = title_element.get('href') if title_element else \"\"\n",
    "                    \n",
    "                    # Extract snippet\n",
    "                    snippet_element = element.find('a', class_='result__snippet')\n",
    "                    snippet = snippet_element.get_text().strip() if snippet_element else \"No snippet\"\n",
    "                    \n",
    "                    if title and url:\n",
    "                        results.append({\n",
    "                            'title': title,\n",
    "                            'url': url,\n",
    "                            'snippet': snippet\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_webpage_content(self, url: str, max_length: int = 1000) -> str:\n",
    "        \"\"\"\n",
    "        Extract text content from a webpage\n",
    "        \n",
    "        Args:\n",
    "            url: URL to fetch\n",
    "            max_length: Maximum length of content to return\n",
    "        \n",
    "        Returns:\n",
    "            Extracted text content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text content\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up whitespace\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            # Truncate if too long\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length] + \"...\"\n",
    "            \n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error fetching content: {str(e)}\"\n",
    "\n",
    "def web_search_and_retrieve(query: str, max_results: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform web search and retrieve content for RAG\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        max_results: Maximum number of results to process\n",
    "    \n",
    "    Returns:\n",
    "        List of documents with web content for RAG\n",
    "    \"\"\"\n",
    "    searcher = WebSearcher()\n",
    "    \n",
    "    # Search for results\n",
    "    search_results = searcher.search_duckduckgo(query, max_results)\n",
    "    \n",
    "    if not search_results:\n",
    "        return []\n",
    "    \n",
    "    # Get content from each result\n",
    "    web_documents = []\n",
    "    for i, result in enumerate(search_results):\n",
    "        content = searcher.get_webpage_content(result['url'])\n",
    "        \n",
    "        web_doc = {\n",
    "            'id': f\"web_{i+1}\",\n",
    "            'title': result['title'],\n",
    "            'content': content,\n",
    "            'url': result['url'],\n",
    "            'snippet': result['snippet'],\n",
    "            'source': 'web'\n",
    "        }\n",
    "        web_documents.append(web_doc)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return web_documents\n",
    "\n",
    "# Initialize web searcher\n",
    "web_searcher = WebSearcher()\n",
    "print(\"Web search functionality initialized!\")\n",
    "\n",
    "# Test the web search\n",
    "test_web_query = \"latest AI developments 2025\"\n",
    "print(f\"\\nTesting web search with query: '{test_web_query}'\")\n",
    "web_results = web_search_and_retrieve(test_web_query, max_results=2)\n",
    "\n",
    "if web_results:\n",
    "    print(f\"Found {len(web_results)} web results:\")\n",
    "    for doc in web_results:\n",
    "        print(f\"  - {doc['title']}\")\n",
    "        print(f\"    URL: {doc['url']}\")\n",
    "        print(f\"    Content preview: {doc['content'][:150]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No web results found or search failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3466d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hybrid retrieval with query: 'What are the latest developments in artificial intelligence?'\n",
      "\n",
      "Retrieved 4 documents total:\n",
      "  ðŸ“š Machine Learning Basics (Score: 0.3801, Source: local)\n",
      "    Content: Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n",
      "  ðŸ“š Deep Learning Overview (Score: 0.3461, Source: local)\n",
      "    Content: Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  ðŸŒ Bachelor Angewandte KÃ¼nstliche Intelligenz - Dein IU Studium (Score: 0.3161, Source: web)\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=iu.de&ad_provider=bingv7aa&ad_type=txad&click_metadata=uaHil7bY69IPrL2FW1Ne88pF5nm1xnFj_jgyKwFYsuLnuT1vHSgzdHlI4yMLhruKcmui%2DH1yKJ%2D_uilEpxjVUavhb6VwuXqicfieW1W5mwEM9f7nurBPioRbNr2HzfL1.QhOPWIV8%2DqRN5vB1NGBmGw&rut=dd2d4fda1c3e9fac0ab9cff31be55c7f8b669847cc910575b27140a6a2453d78&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8jw6tz4cwKVwPb0LlanKxODVUCUzti0nuQDM0vAiDI%2DnfOMzXhKGoQcgnbscwo7GLS3IWloeJptrmfpNv3jfQebMwfiLFWdiRN1t7BZanOkIZrBrouoE0U1nXSO41AUxz7RBRzUPC5xucmGma0g06Amp3msf9pgTP6gMadErSnzHES8H_6UuN71BtDP61Sek2rKV5lg%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cuaXUuZGUlMmZscCUyZmJhY2hlbG9yJTJmYXBwbGllZC1hcnRpZmljaWFsLWludGVsbGlnZW5jZSUyZiUzZm1zY2xraWQlM2RmNDQ5NGYzM2FmZDUxNzkxNjAxNDI3NWNkNTA4NDdjYiUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZEclMjUyMC0lMjUyMERNJTI1MjAtJTI1MjBBTExFJTI1MjAtJTI1MjBERSUyNTIwLSUyNTIwREUlMjUyMC0lMjUyMEFsbGUlMjUyMFN0dWRpZW5nJWMzJWE0bmdlJTI1MjAtJTI1MjBEU0ElMjZ1dG1fdGVybSUzZGh0dHBzJTI1M0ElMjUyRiUyNTJGd3d3Lml1LmRlJTI1MkZscCUyNTJGYmFjaGVsb3IlMjUyRmFwcGxpZWQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UlMjUyRiUyNnV0bV9jb250ZW50JTNkQkElMjUyMC0lMjUyMEFwcGxpZWQlMjUyMEFydGlmaWNpYWwlMjUyMEludGVsbGlnZW5jZQ%26rlid%3Df4494f33afd517916014275cd50847cb&vqd=4-175900891834992938011072442799369704072&iurl=%7B1%7DIG%3DE6711FD4654F40FBA3FF14FD7C421D7A%26CID%3D16E646E3C686634E3DC750C6C72A6219%26ID%3DDevEx%2C5047.1\n",
      "    Content: Bachelor Angewandte KÃ¼nstliche Intelligenz | Dein IU Studium Zum Hauptinhalt wechselnMit Code SHINE2...\n",
      "\n",
      "  ðŸŒ Intelligente AI-Assistenten | AI-Assistenten nutzen (Score: 0.0397, Source: web)\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=u%2Dexperten.de&ad_provider=bingv7aa&ad_type=txad&click_metadata=R59tHkNKrLOy_hUird1FKdJiQh9fb1gqQY7N2WnVASBzM_qrUfCzRYEIofjgmWZKdUpg%2DRYo7ttJtjelM3W4hlszUFXiQX0etfLLHJ8pWfNgdoJ73dBx2YS9q8uFpteP.io1d7L1TmRRcvwBKu7%2DuVA&rut=a1e59cfdeac2967c2ff90cee2645275123fc1aa65d45054a503f872d0127c596&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8K3juNv9PdFFr2TLlUwLcSjVUCUwZDH99DaJ74Zd7uGBdAe1x0tqp5qW1l0eKipBEP4PRmvUIMzFDlm4Mv73nvgRDM4AmMn1Yx6eX6kybvXtlVEPXp6r_sMmPUCReXzL_ycvxHDWGlWhO8Fks2mCPzs8KA2dxJOTxOLKAgdZ1IlgBeLkYbEZuLiGHp_7hlrWdJ_DXig%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cudS1leHBlcnRlbi5kZSUyZmFpJTJmJTNmZXRjY19tZWQlM2RTRUElMjZldGNjX3BhciUzZEJpbmclMjZldGNjX2NtcCUzZFVYUCUyNTIwQUklMjUyMDIwMjQlMjZldGNjX2dycCUzZEFJJTI2ZXRjY19ia3klM2Rwcm9kdWN0cyUyNTIwd2l0aCUyNTIwQUklMjZldGNjX210eSUzZGIlMjZldGNjX2JkZSUzZGMlMjZldGNjX2N0diUzZDg0ODAwNDA0MDk1NDMyJTI2ZXRjY19rZXklM2RXaGF0JTI1MjBhcmUlMjUyMHRoZSUyNTIwbGF0ZXN0JTI1MjBkZXZlbG9wbWVudHMlMjUyMGluJTI1MjBhcnRpZmljaWFsJTI1MjBpbnRlbGxpZ2VuY2UlMjUzRiUyNmV0X2NtcF9zZWc1JTNkcyUyNmV0Y2NfdmFyJTNkOGM5MjA0ZjFhNjc3MWMyZDEzMWY2YzllZGNhYTlmN2IlMjZtc2Nsa2lkJTNkOGM5MjA0ZjFhNjc3MWMyZDEzMWY2YzllZGNhYTlmN2I%26rlid%3D8c9204f1a6771c2d131f6c9edcaa9f7b&vqd=4-338589950631181380873416503352248576362&iurl=%7B1%7DIG%3DE6711FD4654F40FBA3FF14FD7C421D7A%26CID%3D16E646E3C686634E3DC750C6C72A6219%26ID%3DDevEx%2C5050.1\n",
      "    Content: Error fetching content: ('Connection aborted.', RemoteDisconnected('Remote end closed connection wit...\n",
      "\n",
      "\n",
      "Retrieved 4 documents total:\n",
      "  ðŸ“š Machine Learning Basics (Score: 0.3801, Source: local)\n",
      "    Content: Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n",
      "  ðŸ“š Deep Learning Overview (Score: 0.3461, Source: local)\n",
      "    Content: Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  ðŸŒ Bachelor Angewandte KÃ¼nstliche Intelligenz - Dein IU Studium (Score: 0.3161, Source: web)\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=iu.de&ad_provider=bingv7aa&ad_type=txad&click_metadata=uaHil7bY69IPrL2FW1Ne88pF5nm1xnFj_jgyKwFYsuLnuT1vHSgzdHlI4yMLhruKcmui%2DH1yKJ%2D_uilEpxjVUavhb6VwuXqicfieW1W5mwEM9f7nurBPioRbNr2HzfL1.QhOPWIV8%2DqRN5vB1NGBmGw&rut=dd2d4fda1c3e9fac0ab9cff31be55c7f8b669847cc910575b27140a6a2453d78&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8jw6tz4cwKVwPb0LlanKxODVUCUzti0nuQDM0vAiDI%2DnfOMzXhKGoQcgnbscwo7GLS3IWloeJptrmfpNv3jfQebMwfiLFWdiRN1t7BZanOkIZrBrouoE0U1nXSO41AUxz7RBRzUPC5xucmGma0g06Amp3msf9pgTP6gMadErSnzHES8H_6UuN71BtDP61Sek2rKV5lg%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cuaXUuZGUlMmZscCUyZmJhY2hlbG9yJTJmYXBwbGllZC1hcnRpZmljaWFsLWludGVsbGlnZW5jZSUyZiUzZm1zY2xraWQlM2RmNDQ5NGYzM2FmZDUxNzkxNjAxNDI3NWNkNTA4NDdjYiUyNnV0bV9zb3VyY2UlM2RiaW5nJTI2dXRtX21lZGl1bSUzZGNwYyUyNnV0bV9jYW1wYWlnbiUzZEclMjUyMC0lMjUyMERNJTI1MjAtJTI1MjBBTExFJTI1MjAtJTI1MjBERSUyNTIwLSUyNTIwREUlMjUyMC0lMjUyMEFsbGUlMjUyMFN0dWRpZW5nJWMzJWE0bmdlJTI1MjAtJTI1MjBEU0ElMjZ1dG1fdGVybSUzZGh0dHBzJTI1M0ElMjUyRiUyNTJGd3d3Lml1LmRlJTI1MkZscCUyNTJGYmFjaGVsb3IlMjUyRmFwcGxpZWQtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UlMjUyRiUyNnV0bV9jb250ZW50JTNkQkElMjUyMC0lMjUyMEFwcGxpZWQlMjUyMEFydGlmaWNpYWwlMjUyMEludGVsbGlnZW5jZQ%26rlid%3Df4494f33afd517916014275cd50847cb&vqd=4-175900891834992938011072442799369704072&iurl=%7B1%7DIG%3DE6711FD4654F40FBA3FF14FD7C421D7A%26CID%3D16E646E3C686634E3DC750C6C72A6219%26ID%3DDevEx%2C5047.1\n",
      "    Content: Bachelor Angewandte KÃ¼nstliche Intelligenz | Dein IU Studium Zum Hauptinhalt wechselnMit Code SHINE2...\n",
      "\n",
      "  ðŸŒ Intelligente AI-Assistenten | AI-Assistenten nutzen (Score: 0.0397, Source: web)\n",
      "    URL: https://duckduckgo.com/y.js?ad_domain=u%2Dexperten.de&ad_provider=bingv7aa&ad_type=txad&click_metadata=R59tHkNKrLOy_hUird1FKdJiQh9fb1gqQY7N2WnVASBzM_qrUfCzRYEIofjgmWZKdUpg%2DRYo7ttJtjelM3W4hlszUFXiQX0etfLLHJ8pWfNgdoJ73dBx2YS9q8uFpteP.io1d7L1TmRRcvwBKu7%2DuVA&rut=a1e59cfdeac2967c2ff90cee2645275123fc1aa65d45054a503f872d0127c596&u3=https%3A%2F%2Fwww.bing.com%2Faclick%3Fld%3De8K3juNv9PdFFr2TLlUwLcSjVUCUwZDH99DaJ74Zd7uGBdAe1x0tqp5qW1l0eKipBEP4PRmvUIMzFDlm4Mv73nvgRDM4AmMn1Yx6eX6kybvXtlVEPXp6r_sMmPUCReXzL_ycvxHDWGlWhO8Fks2mCPzs8KA2dxJOTxOLKAgdZ1IlgBeLkYbEZuLiGHp_7hlrWdJ_DXig%26u%3DaHR0cHMlM2ElMmYlMmZ3d3cudS1leHBlcnRlbi5kZSUyZmFpJTJmJTNmZXRjY19tZWQlM2RTRUElMjZldGNjX3BhciUzZEJpbmclMjZldGNjX2NtcCUzZFVYUCUyNTIwQUklMjUyMDIwMjQlMjZldGNjX2dycCUzZEFJJTI2ZXRjY19ia3klM2Rwcm9kdWN0cyUyNTIwd2l0aCUyNTIwQUklMjZldGNjX210eSUzZGIlMjZldGNjX2JkZSUzZGMlMjZldGNjX2N0diUzZDg0ODAwNDA0MDk1NDMyJTI2ZXRjY19rZXklM2RXaGF0JTI1MjBhcmUlMjUyMHRoZSUyNTIwbGF0ZXN0JTI1MjBkZXZlbG9wbWVudHMlMjUyMGluJTI1MjBhcnRpZmljaWFsJTI1MjBpbnRlbGxpZ2VuY2UlMjUzRiUyNmV0X2NtcF9zZWc1JTNkcyUyNmV0Y2NfdmFyJTNkOGM5MjA0ZjFhNjc3MWMyZDEzMWY2YzllZGNhYTlmN2IlMjZtc2Nsa2lkJTNkOGM5MjA0ZjFhNjc3MWMyZDEzMWY2YzllZGNhYTlmN2I%26rlid%3D8c9204f1a6771c2d131f6c9edcaa9f7b&vqd=4-338589950631181380873416503352248576362&iurl=%7B1%7DIG%3DE6711FD4654F40FBA3FF14FD7C421D7A%26CID%3D16E646E3C686634E3DC750C6C72A6219%26ID%3DDevEx%2C5050.1\n",
      "    Content: Error fetching content: ('Connection aborted.', RemoteDisconnected('Remote end closed connection wit...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hybrid RAG: Combine Local Knowledge Base + Web Search\n",
    "def hybrid_retrieve_documents(query: str, local_top_k: int = 2, web_top_k: int = 2, use_web: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from both local knowledge base and web search\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        local_top_k: Number of local documents to retrieve\n",
    "        web_top_k: Number of web documents to retrieve\n",
    "        use_web: Whether to include web search results\n",
    "    \n",
    "    Returns:\n",
    "        Combined list of local and web documents\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Get local documents\n",
    "    local_docs = retrieve_relevant_documents(query, local_top_k)\n",
    "    for doc in local_docs:\n",
    "        doc['source'] = 'local'\n",
    "        all_documents.append(doc)\n",
    "    \n",
    "    # Get web documents if enabled\n",
    "    if use_web:\n",
    "        try:\n",
    "            web_docs = web_search_and_retrieve(query, web_top_k)\n",
    "            \n",
    "            # Add embeddings for web documents to enable similarity scoring\n",
    "            if web_docs:\n",
    "                web_contents = [doc['content'] for doc in web_docs]\n",
    "                web_embeddings = embedding_model.encode(web_contents)\n",
    "                query_embedding = embedding_model.encode([query])\n",
    "                \n",
    "                # Calculate similarity scores\n",
    "                for i, doc in enumerate(web_docs):\n",
    "                    similarity = float(np.dot(query_embedding[0], web_embeddings[i]) / \n",
    "                                     (np.linalg.norm(query_embedding[0]) * np.linalg.norm(web_embeddings[i])))\n",
    "                    doc['relevance_score'] = similarity\n",
    "                    doc['rank'] = len(all_documents) + i + 1\n",
    "                    all_documents.append(doc)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Web search failed: {str(e)}\")\n",
    "    \n",
    "    # Sort all documents by relevance score\n",
    "    all_documents.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "    \n",
    "    # Re-rank\n",
    "    for i, doc in enumerate(all_documents):\n",
    "        doc['rank'] = i + 1\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# Test hybrid retrieval\n",
    "test_hybrid_query = \"What are the latest developments in artificial intelligence?\"\n",
    "print(f\"Testing hybrid retrieval with query: '{test_hybrid_query}'\")\n",
    "hybrid_results = hybrid_retrieve_documents(test_hybrid_query, local_top_k=2, web_top_k=2, use_web=True)\n",
    "\n",
    "print(f\"\\nRetrieved {len(hybrid_results)} documents total:\")\n",
    "for doc in hybrid_results:\n",
    "    source_icon = \"ðŸŒ\" if doc['source'] == 'web' else \"ðŸ“š\"\n",
    "    print(f\"  {source_icon} {doc['title']} (Score: {doc['relevance_score']:.4f}, Source: {doc['source']})\")\n",
    "    if doc['source'] == 'web':\n",
    "        print(f\"    URL: {doc['url']}\")\n",
    "    print(f\"    Content: {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40d4dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ EXAMPLE 3: WEB-ENHANCED RAG INTERFACE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d100a2b51da14b1cbe58b5e89c41fa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What are the latest developments in artificial intelligence and machine learninâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Web-Enhanced RAG System Ready!\n",
      "This is the most advanced interface - combining local knowledge with real-time web search!\n",
      "Ask questions about current events, latest developments, or any topic!\n",
      "The system will intelligently search both local knowledge and the web for the most relevant information.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Web-Enhanced RAG Interface\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox, IntSlider, Tab\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"ðŸŒ EXAMPLE 3: WEB-ENHANCED RAG INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create input area\n",
    "web_input_box = Textarea(\n",
    "    value='What are the latest developments in artificial intelligence and machine learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '700px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create checkboxes for different modes\n",
    "local_rag_checkbox = Checkbox(value=True, description='Use Local Knowledge Base')\n",
    "web_search_checkbox = Checkbox(value=True, description='Use Web Search')\n",
    "\n",
    "# Create sliders for controlling retrieval\n",
    "local_docs_slider = IntSlider(value=2, min=1, max=5, description='Local Docs:')\n",
    "web_docs_slider = IntSlider(value=2, min=1, max=5, description='Web Docs:')\n",
    "\n",
    "# Create buttons\n",
    "web_generate_button = Button(description='ðŸ” Generate with Web Search', button_style='success')\n",
    "web_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create output area\n",
    "web_output_area = Output()\n",
    "\n",
    "# Arrange widgets\n",
    "mode_controls = HBox([local_rag_checkbox, web_search_checkbox])\n",
    "doc_controls = HBox([local_docs_slider, web_docs_slider])\n",
    "button_controls = HBox([web_generate_button, web_clear_button])\n",
    "web_enhanced_ui = VBox([web_input_box, mode_controls, doc_controls, button_controls, web_output_area])\n",
    "\n",
    "display(web_enhanced_ui)\n",
    "\n",
    "def generate_web_enhanced_response(query: str, use_local: bool = True, use_web: bool = True, \n",
    "                                 local_docs: int = 2, web_docs: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Generate response using hybrid RAG with web search\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    all_sources = []\n",
    "    \n",
    "    if use_local or use_web:\n",
    "        # Get hybrid results\n",
    "        if use_local and use_web:\n",
    "            relevant_docs = hybrid_retrieve_documents(query, local_docs, web_docs, use_web=True)\n",
    "        elif use_local:\n",
    "            relevant_docs = retrieve_relevant_documents(query, local_docs)\n",
    "            for doc in relevant_docs:\n",
    "                doc['source'] = 'local'\n",
    "        elif use_web:\n",
    "            relevant_docs = web_search_and_retrieve(query, web_docs)\n",
    "            # Calculate relevance scores for web-only documents\n",
    "            if relevant_docs:\n",
    "                web_contents = [doc['content'] for doc in relevant_docs]\n",
    "                web_embeddings = embedding_model.encode(web_contents)\n",
    "                query_embedding = embedding_model.encode([query])\n",
    "                \n",
    "                for i, doc in enumerate(relevant_docs):\n",
    "                    similarity = float(np.dot(query_embedding[0], web_embeddings[i]) / \n",
    "                                     (np.linalg.norm(query_embedding[0]) * np.linalg.norm(web_embeddings[i])))\n",
    "                    doc['relevance_score'] = similarity\n",
    "                    doc['source'] = 'web'\n",
    "        else:\n",
    "            relevant_docs = []\n",
    "        \n",
    "        # Build context from all sources\n",
    "        for i, doc in enumerate(relevant_docs):\n",
    "            source_label = \"Local Knowledge\" if doc['source'] == 'local' else \"Web Search\"\n",
    "            context_parts.append(f\"{source_label} {i+1}: {doc['content']}\")\n",
    "            all_sources.append(doc)\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread who has become surprisingly knowledgeable about technology and current events. You maintain your dry, sardonic personality while providing informative and accurate answers.\n",
    "\n",
    "Use the following context from multiple sources to answer the user's question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context from both local knowledge and web sources. Cite your sources when relevant, and add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "        relevant_docs = []\n",
    "    \n",
    "    # Generate response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs\n",
    "\n",
    "def web_enhanced_generate_response(_):\n",
    "    web_output_area.clear_output()\n",
    "    \n",
    "    query = web_input_box.value\n",
    "    use_local = local_rag_checkbox.value\n",
    "    use_web = web_search_checkbox.value\n",
    "    local_docs = local_docs_slider.value\n",
    "    web_docs = web_docs_slider.value\n",
    "    \n",
    "    with web_output_area:\n",
    "        # Display query info\n",
    "        print(\"ðŸ” WEB-ENHANCED RAG SYSTEM\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Local Knowledge: {'âœ“' if use_local else 'âœ—'}\")\n",
    "        print(f\"Web Search: {'âœ“' if use_web else 'âœ—'}\")\n",
    "        print(f\"Local Docs: {local_docs}, Web Docs: {web_docs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_local or use_web:\n",
    "            print(\"ðŸ” Retrieving information...\")\n",
    "            \n",
    "        try:\n",
    "            response, sources = generate_web_enhanced_response(\n",
    "                query, use_local, use_web, local_docs, web_docs\n",
    "            )\n",
    "            \n",
    "            # Display sources\n",
    "            if sources:\n",
    "                print(\"\\nðŸ“š SOURCES CONSULTED:\")\n",
    "                local_count = sum(1 for s in sources if s['source'] == 'local')\n",
    "                web_count = sum(1 for s in sources if s['source'] == 'web')\n",
    "                \n",
    "                print(f\"  ðŸ“š Local Sources: {local_count}\")\n",
    "                print(f\"  ðŸŒ Web Sources: {web_count}\")\n",
    "                print()\n",
    "                \n",
    "                for i, source in enumerate(sources):\n",
    "                    icon = \"ðŸŒ\" if source['source'] == 'web' else \"ðŸ“š\"\n",
    "                    score = source.get('relevance_score', 0.0)  # Use .get() with default\n",
    "                    print(f\"  {icon} {source['title']} (Score: {score:.3f})\")\n",
    "                    if source['source'] == 'web' and 'url' in source:\n",
    "                        print(f\"    URL: {source['url']}\")\n",
    "            \n",
    "            print(\"\\nðŸž BERND'S RESPONSE:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def web_enhanced_clear_output(_):\n",
    "    web_output_area.clear_output()\n",
    "\n",
    "# Connect button events\n",
    "web_generate_button.on_click(web_enhanced_generate_response)\n",
    "web_clear_button.on_click(web_enhanced_clear_output)\n",
    "\n",
    "print(\"ðŸŒ Web-Enhanced RAG System Ready!\")\n",
    "print(\"This is the most advanced interface - combining local knowledge with real-time web search!\")\n",
    "print(\"Ask questions about current events, latest developments, or any topic!\")\n",
    "print(\"The system will intelligently search both local knowledge and the web for the most relevant information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8ea4b",
   "metadata": {},
   "source": [
    "# ðŸŒ Web Search Integration - NEW FEATURE!\n",
    "\n",
    "## What's New: Real-Time Web Search for RAG\n",
    "\n",
    "The notebook now includes **web search functionality** that allows Bernd the Bread to access real-time information from the internet! This creates a powerful hybrid RAG system that combines:\n",
    "\n",
    "- **Local Knowledge Base**: Pre-loaded documents about AI, ML, and ZeMA\n",
    "- **Real-Time Web Search**: Live search results from DuckDuckGo (no API keys needed!)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Technical Implementation\n",
    "\n",
    "### Web Search Features:\n",
    "- **DuckDuckGo Integration**: No API keys or rate limits\n",
    "- **Content Extraction**: Automatically extracts text from web pages\n",
    "- **Similarity Scoring**: Ranks web content using embeddings\n",
    "- **Respectful Crawling**: Includes delays and proper user agents\n",
    "\n",
    "### Hybrid RAG System:\n",
    "- **Source Diversity**: Combines local and web sources\n",
    "- **Relevance Ranking**: Scores and ranks all sources together\n",
    "- **Flexible Control**: Toggle local/web sources independently\n",
    "- **Source Attribution**: Clear indication of information sources\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Usage Examples\n",
    "\n",
    "### Try These Queries with Web Search:\n",
    "\n",
    "1. **\"What are the latest AI developments in 2024?\"**\n",
    "   - Tests real-time web search for current events\n",
    "\n",
    "2. **\"How does RAG work and what are recent improvements?\"**\n",
    "   - Combines local RAG knowledge with latest web research\n",
    "\n",
    "3. **\"What's new in large language models this year?\"**\n",
    "   - Gets current information about LLM advances\n",
    "\n",
    "4. **\"Compare GPT-4 with other recent language models\"**\n",
    "   - Searches for comparative information online\n",
    "\n",
    "### Compare Different Modes:\n",
    "\n",
    "- **Local Only**: Use just the built-in knowledge base\n",
    "- **Web Only**: Search only the internet\n",
    "- **Hybrid**: Combine both sources (recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ›ï¸ UI Controls\n",
    "\n",
    "### New Enhanced Interface Features:\n",
    "\n",
    "- **Source Selection**: Toggle local knowledge and web search\n",
    "- **Document Control**: Adjust number of documents from each source\n",
    "- **Real-Time Results**: See which sources were consulted\n",
    "- **Source Attribution**: URLs and relevance scores displayed\n",
    "\n",
    "### Control Sliders:\n",
    "- **Local Docs**: 1-5 documents from knowledge base\n",
    "- **Web Docs**: 1-5 documents from web search\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Technical Details\n",
    "\n",
    "### Dependencies Added:\n",
    "- `requests`: For web requests\n",
    "- `beautifulsoup4`: For HTML parsing\n",
    "- `urllib.parse`: For URL handling\n",
    "\n",
    "### Search Process:\n",
    "1. **Query Processing**: User question analyzed\n",
    "2. **Local Retrieval**: Search knowledge base using embeddings\n",
    "3. **Web Search**: Query DuckDuckGo for relevant pages\n",
    "4. **Content Extraction**: Parse and clean web page content\n",
    "5. **Similarity Scoring**: Rank all sources by relevance\n",
    "6. **Context Building**: Combine sources for LLM context\n",
    "7. **Response Generation**: Generate informed response\n",
    "\n",
    "### Error Handling:\n",
    "- Graceful fallback if web search fails\n",
    "- Timeout protection for web requests\n",
    "- Content length limits to prevent overload\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Benefits of Web Search Integration\n",
    "\n",
    "1. **Current Information**: Access to real-time data and recent developments\n",
    "2. **Broader Knowledge**: Not limited to pre-loaded knowledge base\n",
    "3. **Fact Verification**: Cross-reference information from multiple sources\n",
    "4. **Dynamic Updates**: No need to manually update knowledge base\n",
    "5. **Source Transparency**: Clear indication of information sources\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Usage Tips\n",
    "\n",
    "1. **Start with Hybrid Mode**: Use both local and web sources for best results\n",
    "2. **Adjust Document Counts**: More documents = more context but slower processing\n",
    "3. **Check Sources**: Review which sources were used for each response\n",
    "4. **Compare Modes**: Try the same query with different source combinations\n",
    "5. **Be Patient**: Web search adds processing time but provides fresher information\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Evolution of the System\n",
    "\n",
    "1. **Original**: Simple LLM chat interface\n",
    "2. **RAG Enhanced**: Added local knowledge base retrieval\n",
    "3. **Web-Enhanced RAG**: Added real-time web search capability\n",
    "4. **Hybrid System**: Intelligent combination of all sources\n",
    "\n",
    "This creates a powerful, flexible AI assistant that can answer questions using both curated knowledge and real-time web information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5fbe1",
   "metadata": {},
   "source": [
    "# ðŸ“š Complete Guide to Progressive LLM Enhancement\n",
    "\n",
    "This notebook demonstrates the complete evolution of LLM applications through three progressive stages:\n",
    "\n",
    "## ðŸŽ¯ **The Three-Stage Journey:**\n",
    "\n",
    "### ðŸž **Example 1: Simple Chat Interface**\n",
    "- **Location**: Cell 4\n",
    "- **Purpose**: Basic chat interface with Bernd the Bread\n",
    "- **Knowledge**: Only the base model's training data\n",
    "- **Best for**: General conversation and creative tasks\n",
    "- **Try**: \"Tell me about artificial intelligence\"\n",
    "\n",
    "### ðŸ“š **Example 2: RAG-Enhanced Interface**\n",
    "- **Location**: Cell 5  \n",
    "- **Purpose**: Adds local knowledge base retrieval\n",
    "- **Knowledge**: Model training data + curated documents\n",
    "- **Best for**: Specific domain questions about AI, ML, ZeMA\n",
    "- **Try**: \"What is the difference between machine learning and deep learning?\"\n",
    "- **Toggle**: RAG on/off to compare responses\n",
    "\n",
    "### ðŸŒ **Example 3: Web-Enhanced RAG Interface**\n",
    "- **Location**: Cell 8\n",
    "- **Purpose**: Hybrid RAG with real-time web search\n",
    "- **Knowledge**: Model + local docs + live web results\n",
    "- **Best for**: Current events, latest developments, comprehensive research\n",
    "- **Try**: \"What are the latest AI developments in 2024?\"\n",
    "- **Controls**: Toggle local/web sources, adjust document counts\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ **Progressive Testing Strategy:**\n",
    "\n",
    "### **Step 1: Baseline (Example 1)**\n",
    "Ask these questions in the simple interface:\n",
    "- \"What is machine learning?\"\n",
    "- \"What is ZeMA?\"  \n",
    "- \"What are the latest AI developments?\"\n",
    "\n",
    "### **Step 2: Enhanced Knowledge (Example 2)**\n",
    "Ask the same questions with RAG enabled:\n",
    "- Notice improved accuracy for domain-specific topics\n",
    "- Toggle RAG on/off to see the difference\n",
    "- Observe document retrieval and relevance scores\n",
    "\n",
    "### **Step 3: Real-Time Information (Example 3)**\n",
    "Ask the same questions with web search:\n",
    "- See how current information enhances responses\n",
    "- Try different combinations: local-only, web-only, hybrid\n",
    "- Notice source attribution and URLs\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ **Key Learning Points:**\n",
    "\n",
    "1. **Simple Model**: Fast but limited to training data\n",
    "2. **RAG Enhancement**: Adds domain expertise but static knowledge\n",
    "3. **Web Integration**: Provides current information but adds complexity\n",
    "\n",
    "## ðŸ› ï¸ **Technical Features:**\n",
    "\n",
    "- **No API Keys**: Everything runs locally or uses free services\n",
    "- **Modular Design**: Each example builds on the previous\n",
    "- **Source Attribution**: Clear indication of information sources\n",
    "- **Flexible Controls**: Adjust retrieval parameters for each mode\n",
    "- **Error Handling**: Graceful fallbacks if components fail\n",
    "\n",
    "## ðŸ“ˆ **Performance Comparison:**\n",
    "\n",
    "| Feature | Example 1 | Example 2 | Example 3 |\n",
    "|---------|-----------|-----------|-----------|\n",
    "| Speed | âš¡ Fastest | ðŸ”„ Medium | ðŸŒ Slower |\n",
    "| Accuracy | ðŸ“Š Basic | ðŸ“š Good | ðŸŽ¯ Excellent |\n",
    "| Currency | âŒ Static | âŒ Static | âœ… Real-time |\n",
    "| Coverage | ðŸ”’ Limited | ðŸ“– Domain | ðŸŒ Global |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ **Usage Instructions:**\n",
    "\n",
    "1. **Run Setup Cells** (1-3): Load model and create knowledge base\n",
    "2. **Try Example 1**: Experience basic LLM interaction\n",
    "3. **Try Example 2**: See how RAG improves domain knowledge\n",
    "4. **Try Example 3**: Experience the full power of web-enhanced RAG\n",
    "5. **Compare Results**: Use the same queries across all three examples\n",
    "\n",
    "This progression shows how modern LLM applications evolve from simple chat to sophisticated, knowledge-enhanced systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35989a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
