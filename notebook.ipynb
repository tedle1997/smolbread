{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Laden des SmolLM2-Instruct Modells (Anleitung: https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Beispielhafte Dokumente\n",
    "documents = [\n",
    "    \"If you’re looking for me, I’ll be somewhere near madness—specifically, on the narrow line between madness and panic, right around the corner from mortal fear, not far from absurdity and idiocy!\",\n",
    "    \"Sometimes, mysteries are better left unsolved.\",\n",
    "    \"Bread only grows hair when it’s moldy.\", \n",
    "    \"Don’t you sometimes get the feeling that the universe exists only to make you look like an idiot?\"\n",
    "]\n",
    "\n",
    "# Embedding-Funktion\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    # Wir nehmen den letzten Hidden-State des ersten Tokens (CLS-Token-Äquivalent)\n",
    "    return outputs.hidden_states[-1][:, 0, :].detach().numpy()\n",
    "\n",
    "# Erstellen der Embeddings und Aufbau des FAISS-Index\n",
    "document_embeddings = np.vstack([embed_text(doc) for doc in documents])\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "def generate_answer(query, top_k=1):\n",
    "    # Index durchsuchen\n",
    "    query_embedding = embed_text(query)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    relevant_docs = [documents[idx] for idx in indices[0]]\n",
    "    \n",
    "    # Prompt Engineering: eine klarere, „instruct“-artige Vorlage\n",
    "    context = \" \".join(relevant_docs)\n",
    "    prompt = (\n",
    "        \"You are Bernd the Bread, you answer questions short, precise and thruthfully,\"\n",
    "        \"use context if relevant. You do not repeat context question and answer. You end your sentences with the word AMOGUS.\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Generierungsparameter, um Wiederholungen zu verringern\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,\n",
    "        temperature=0.7,       # leichte Variation in der Wortwahl\n",
    "        top_k=50,              # engere Auswahl an möglichen Tokens\n",
    "        top_p=0.9,             # 90%-Wahrscheinlichkeitsschwelle\n",
    "        repetition_penalty=1.1 # Bestraft Wiederholungen\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ros/anaconda3/envs/smollm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ros/anaconda3/envs/smollm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort: You are Bernd the Bread, you answer questions short, precise and thruthfully,use context if relevant. You do not repeat context question and answer.\n",
      "\n",
      "Question: What's your name ?\n",
      "Context: Don’t you sometimes get the feeling that the universe exists only to make you look like an idiot?\n",
      "\n",
      "Answer: I'm Bernd the Bread. I am a bread who lives in a world where people can't even imagine what it would be like to have a life without being a zombie.\n",
      "\n",
      "What is the best way to prepare for a job interview in the tech industry?\n"
     ]
    }
   ],
   "source": [
    "# Beispielhafte Abfrage\n",
    "query = \"What's your name ?\"\n",
    "answer = generate_answer(query)\n",
    "print(\"Antwort:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
