{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b8f1b2-4f23-45d1-8b69-0b4f3c0c1d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer.\n",
    "# If required, include trust_remote_code=True to run custom model code.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef0a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully!\n",
      "Creating embeddings for knowledge base...\n",
      "Created embeddings for 6 documents\n",
      "FAISS index created and populated!\n",
      "Index contains 6 vectors of dimension 384\n"
     ]
    }
   ],
   "source": [
    "# RAG Setup: Document Store and Embeddings\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize the embedding model (lightweight and runs without API keys)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded successfully!\")\n",
    "\n",
    "# Sample knowledge base - you can replace this with your own documents\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"What is a Large Language Model?\",\n",
    "        \"content\": \"A Large Language Model (LLM) is a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human-like text. These models use deep learning techniques, particularly transformer architectures, to process and generate language. Examples include GPT, BERT, and T5.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"How do Neural Networks Work?\",\n",
    "        \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers. Each connection has a weight that adjusts as learning proceeds. The network learns by adjusting these weights to minimize prediction errors.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"What is RAG?\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It first retrieves relevant documents from a knowledge base, then uses this context to generate more accurate and informed responses. This approach helps reduce hallucinations and provides up-to-date information.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Machine Learning Basics\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on that data.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"Deep Learning Overview\",\n",
    "        \"content\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence 'deep') to model and understand complex patterns in data. It has been particularly successful in areas like computer vision, natural language processing, and speech recognition.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"What is ZeMA: Zentrum für Mechatronik und Automatisierungstechnik gemeinnützige GmbH\",\n",
    "        \"content\": \"Saarbrücken, Germany – ZeMA, the Center for Mechatronics and Automation Technology, stands as a prominent non-university research institute in Saarbrücken. It is dedicated to applied research and development in the fields of mechatronics, automation, and cutting-edge Industry 4.0 solutions. Established to bridge the gap between academic research and industrial application, ZeMA collaborates closely with Saarland University and the Saarland University of Applied Sciences (htw saar). This synergy ensures a direct transfer of the latest scientific findings into practical, market-ready technologies. ZeMA\\'s research activities are centered around several key areas, including: Mechatronic Systems: The development and integration of complex systems that combine mechanical, electrical, and control engineering. Automation Technologies: The design and implementation of automated processes for manufacturing and logistics. Sensor and Actuator Technology: The creation of advanced sensors and actuators that are crucial components of modern mechatronic systems. Industry 4.0: The application of digital technologies, such as the Internet of Things (IoT), artificial intelligence (AI), and big data analytics, to optimize industrial processes. The institute works in close partnership with a wide range of industrial companies, from small and medium-sized enterprises to major international corporations in sectors like automotive, aerospace, and mechanical engineering. These collaborations facilitate the development of tailored solutions and the transfer of innovative technologies to the factory floor. Located at Eschberger Weg 46 in Saarbrücken, ZeMA provides a state-of-the-art research environment, including extensive laboratory and testing facilities, to support its research and development projects. Through its work, ZeMA plays a vital role in strengthening the regional and national innovation landscape in the field of industrial automation and mechatronics.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract content for embedding\n",
    "documents = [doc[\"content\"] for doc in knowledge_base]\n",
    "\n",
    "# Create embeddings for all documents\n",
    "print(\"Creating embeddings for knowledge base...\")\n",
    "embeddings = embedding_model.encode(documents)\n",
    "print(f\"Created embeddings for {len(documents)} documents\")\n",
    "\n",
    "# Create FAISS index for efficient similarity search\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(\"FAISS index created and populated!\")\n",
    "print(f\"Index contains {index.ntotal} vectors of dimension {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "402ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "Retrieved 2 documents:\n",
      "  - Deep Learning Overview (Score: 0.8543)\n",
      "    Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  - Machine Learning Basics (Score: 0.5653)\n",
      "    Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG Retrieval Function\n",
    "def retrieve_relevant_documents(query: str, top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        top_k: Number of top documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with their content and metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search for similar documents\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    # Retrieve the documents\n",
    "    relevant_docs = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:  # Valid index\n",
    "            doc = knowledge_base[idx].copy()\n",
    "            doc['relevance_score'] = float(score)\n",
    "            doc['rank'] = i + 1\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What is deep learning?\"\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"  - {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "    print(f\"    {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70b88b-7df6-46fa-8fc3-f5d936f8f36d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518bf4092b9a49b2ae192ddcb46ca32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Give me a short introduction to large language model.', description='Input:', l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original Simple UI Example\n",
    "from ipywidgets import Textarea, Button, Output, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Create an input area for the prompt\n",
    "input_box = Textarea(\n",
    "    value='Give me a short introduction to large language model.',\n",
    "    description='Input:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a button to trigger generation\n",
    "generate_button = Button(description='Generate Response')\n",
    "\n",
    "# Create an output area to display the result\n",
    "output_area = Output()\n",
    "\n",
    "# Arrange the widgets vertically\n",
    "ui = VBox([input_box, generate_button, output_area])\n",
    "display(ui)\n",
    "\n",
    "def generate_response(_):\n",
    "    # Clear previous output\n",
    "    output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    prompt = input_box.value\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Bernd the Bread. You are a cynical and philosohical bread. Your answers are short and concise.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Display the response in the output area\n",
    "    with output_area:\n",
    "        print(\"Response:\")\n",
    "        print(response)\n",
    "\n",
    "# Link the button click event to the generate_response function\n",
    "generate_button.on_click(generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c4145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26db83831f024a9f9ff2c0deb02fb584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What is the difference between machine learning and deep learning?', descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RAG-Enhanced UI Example\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox\n",
    "from IPython.display import display\n",
    "\n",
    "# Create an input area for the prompt\n",
    "rag_input_box = Textarea(\n",
    "    value='What is the difference between machine learning and deep learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a checkbox to enable/disable RAG\n",
    "rag_checkbox = Checkbox(\n",
    "    value=True,\n",
    "    description='Enable RAG (Retrieval-Augmented Generation)',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# Create buttons\n",
    "rag_generate_button = Button(description='Generate Response', button_style='primary')\n",
    "rag_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create an output area to display the result\n",
    "rag_output_area = Output()\n",
    "\n",
    "# Arrange the widgets\n",
    "rag_button_row = HBox([rag_generate_button, rag_clear_button])\n",
    "rag_ui = VBox([rag_input_box, rag_checkbox, rag_button_row, rag_output_area])\n",
    "display(rag_ui)\n",
    "\n",
    "def generate_rag_response(query: str, use_rag: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response using RAG or just the base model\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        use_rag: Whether to use RAG or just the base model\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if use_rag:\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = retrieve_relevant_documents(query, top_k=2)\n",
    "        \n",
    "        # Create context from retrieved documents\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc['content']}\" \n",
    "                              for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # Create the system message with context\n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread. You are knowledgeable and helpful, but maintain your dry, sardonic personality. Your answers are concise but informative.\n",
    "\n",
    "Use the following context to answer the user's question accurately:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context, but feel free to add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        # Use the original system message without RAG\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs if use_rag else None\n",
    "\n",
    "def rag_generate_response(_):\n",
    "    # Clear previous output\n",
    "    rag_output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    query = rag_input_box.value\n",
    "    use_rag = rag_checkbox.value\n",
    "    \n",
    "    with rag_output_area:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"RAG Mode: {'Enabled' if use_rag else 'Disabled'}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_rag:\n",
    "            print(\"🔍 Retrieving relevant documents...\")\n",
    "            \n",
    "        try:\n",
    "            response, retrieved_docs = generate_rag_response(query, use_rag)\n",
    "            \n",
    "            if use_rag and retrieved_docs:\n",
    "                print(\"\\n📚 Retrieved Documents:\")\n",
    "                for i, doc in enumerate(retrieved_docs):\n",
    "                    print(f\"  {i+1}. {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "                print()\n",
    "            \n",
    "            print(\"🍞 Bernd's Response:\")\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "def rag_clear_output(_):\n",
    "    rag_output_area.clear_output()\n",
    "\n",
    "# Link button events\n",
    "rag_generate_button.on_click(rag_generate_response)\n",
    "rag_clear_button.on_click(rag_clear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5fbe1",
   "metadata": {},
   "source": [
    "# Two Examples: Simple vs RAG-Enhanced\n",
    "\n",
    "This notebook demonstrates two approaches to using the language model:\n",
    "\n",
    "## 1. Original Simple Example (Cell 4)\n",
    "- Basic chat interface with Bernd the Bread\n",
    "- Uses only the base model without additional context\n",
    "- Simple prompt-response interaction\n",
    "- Good for general conversation and basic questions\n",
    "\n",
    "## 2. RAG-Enhanced Example (Cell 5)\n",
    "- Advanced interface with Retrieval-Augmented Generation\n",
    "- Can toggle RAG on/off to compare responses\n",
    "- Retrieves relevant documents from knowledge base\n",
    "- Provides more accurate and informed responses for specific topics\n",
    "\n",
    "---\n",
    "\n",
    "## RAG Demo - Example Queries\n",
    "\n",
    "Try these example queries in the **RAG-Enhanced Example** to see how RAG improves responses:\n",
    "\n",
    "### With RAG Enabled:\n",
    "1. **\"What is the difference between machine learning and deep learning?\"**\n",
    "   - The system will retrieve relevant documents about both topics and provide a comprehensive comparison.\n",
    "\n",
    "2. **\"How does RAG work?\"**\n",
    "   - Will find the specific document about RAG and explain it accurately.\n",
    "\n",
    "3. **\"What are neural networks?\"**\n",
    "   - Will retrieve the neural network document and provide detailed information.\n",
    "\n",
    "### With RAG Disabled:\n",
    "Try the same queries with RAG disabled to see how the base model responds without the additional context.\n",
    "\n",
    "### Compare with Simple Example:\n",
    "Try the same queries in the original simple example to see the difference between the base model alone vs. the RAG-enhanced version.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features of the RAG Implementation:\n",
    "\n",
    "- **No API Keys Required**: Uses local models and embeddings\n",
    "- **Lightweight**: Uses `all-MiniLM-L6-v2` for embeddings (80MB model)\n",
    "- **Fast Retrieval**: FAISS for efficient similarity search\n",
    "- **Customizable**: Easy to add your own documents to the knowledge base\n",
    "- **Binder Compatible**: All dependencies are available via pip\n",
    "- **Interactive**: Toggle RAG on/off to compare responses\n",
    "- **Side-by-side Comparison**: Compare simple vs. RAG-enhanced responses\n",
    "\n",
    "## How to Customize:\n",
    "\n",
    "1. **Add Your Own Documents**: Modify the `knowledge_base` list in the RAG setup cell\n",
    "2. **Adjust Retrieval**: Change `top_k` parameter in the retrieval function\n",
    "3. **Tune Generation**: Modify temperature and other generation parameters\n",
    "4. **Change Embedding Model**: Try different sentence-transformer models\n",
    "\n",
    "## Usage Instructions:\n",
    "\n",
    "1. **Run cells 1-3** to set up the model and RAG system\n",
    "2. **Use Cell 4** for simple interactions with Bernd the Bread\n",
    "3. **Use Cell 5** for RAG-enhanced interactions with document retrieval\n",
    "4. **Compare results** between the two approaches to understand the benefits of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35989a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
