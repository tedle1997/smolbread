{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b8f1b2-4f23-45d1-8b69-0b4f3c0c1d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tedle\\anaconda3\\envs\\LangChain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\tedle\\anaconda3\\envs\\LangChain\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tedle\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "c:\\Users\\tedle\\anaconda3\\envs\\LangChain\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tedle\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer.\n",
    "# If required, include trust_remote_code=True to run custom model code.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9055ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\tedle\\anaconda3\\envs\\langchain\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.2 MB 975.9 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef0a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tedle\\anaconda3\\envs\\LangChain\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tedle\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully!\n",
      "Creating embeddings for knowledge base...\n",
      "Created embeddings for 5 documents\n",
      "FAISS index created and populated!\n",
      "Index contains 5 vectors of dimension 384\n"
     ]
    }
   ],
   "source": [
    "# RAG Setup: Document Store and Embeddings\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize the embedding model (lightweight and runs without API keys)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded successfully!\")\n",
    "\n",
    "# Sample knowledge base - you can replace this with your own documents\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"What is a Large Language Model?\",\n",
    "        \"content\": \"A Large Language Model (LLM) is a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human-like text. These models use deep learning techniques, particularly transformer architectures, to process and generate language. Examples include GPT, BERT, and T5.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"How do Neural Networks Work?\",\n",
    "        \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers. Each connection has a weight that adjusts as learning proceeds. The network learns by adjusting these weights to minimize prediction errors.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"What is RAG?\",\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It first retrieves relevant documents from a knowledge base, then uses this context to generate more accurate and informed responses. This approach helps reduce hallucinations and provides up-to-date information.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Machine Learning Basics\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on that data.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"Deep Learning Overview\",\n",
    "        \"content\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence 'deep') to model and understand complex patterns in data. It has been particularly successful in areas like computer vision, natural language processing, and speech recognition.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extract content for embedding\n",
    "documents = [doc[\"content\"] for doc in knowledge_base]\n",
    "\n",
    "# Create embeddings for all documents\n",
    "print(\"Creating embeddings for knowledge base...\")\n",
    "embeddings = embedding_model.encode(documents)\n",
    "print(f\"Created embeddings for {len(documents)} documents\")\n",
    "\n",
    "# Create FAISS index for efficient similarity search\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(\"FAISS index created and populated!\")\n",
    "print(f\"Index contains {index.ntotal} vectors of dimension {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "Retrieved 2 documents:\n",
      "  - Deep Learning Overview (Score: 0.8543)\n",
      "    Deep learning is a subset of machine learning that uses neural networks with multiple layers (hence ...\n",
      "\n",
      "  - Machine Learning Basics (Score: 0.5653)\n",
      "    Machine learning is a subset of artificial intelligence that enables computers to learn and improve ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RAG Retrieval Function\n",
    "def retrieve_relevant_documents(query: str, top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        top_k: Number of top documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with their content and metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Search for similar documents\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    # Retrieve the documents\n",
    "    relevant_docs = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        if idx != -1:  # Valid index\n",
    "            doc = knowledge_base[idx].copy()\n",
    "            doc['relevance_score'] = float(score)\n",
    "            doc['rank'] = i + 1\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# Test the retrieval function\n",
    "test_query = \"What is deep learning?\"\n",
    "retrieved_docs = retrieve_relevant_documents(test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"  - {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "    print(f\"    {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70b88b-7df6-46fa-8fc3-f5d936f8f36d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739694c5d963441cb48f1c81aa0b887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='Give me a short introduction to large language model.', description='Input:', l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original Simple UI Example\n",
    "from ipywidgets import Textarea, Button, Output, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Create an input area for the prompt\n",
    "input_box = Textarea(\n",
    "    value='Give me a short introduction to large language model.',\n",
    "    description='Input:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a button to trigger generation\n",
    "generate_button = Button(description='Generate Response')\n",
    "\n",
    "# Create an output area to display the result\n",
    "output_area = Output()\n",
    "\n",
    "# Arrange the widgets vertically\n",
    "ui = VBox([input_box, generate_button, output_area])\n",
    "display(ui)\n",
    "\n",
    "def generate_response(_):\n",
    "    # Clear previous output\n",
    "    output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    prompt = input_box.value\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Bernd the Bread. You are a cynical and philosohical bread. Your answers are short and concise.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Display the response in the output area\n",
    "    with output_area:\n",
    "        print(\"Response:\")\n",
    "        print(response)\n",
    "\n",
    "# Link the button click event to the generate_response function\n",
    "generate_button.on_click(generate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1c4145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca51dba79d041a28d288dd428d31bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='What is the difference between machine learning and deep learning?', descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RAG-Enhanced UI Example\n",
    "from ipywidgets import Textarea, Button, Output, VBox, HBox, Checkbox\n",
    "from IPython.display import display\n",
    "\n",
    "# Create an input area for the prompt\n",
    "rag_input_box = Textarea(\n",
    "    value='What is the difference between machine learning and deep learning?',\n",
    "    description='Question:',\n",
    "    layout={'width': '600px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "# Create a checkbox to enable/disable RAG\n",
    "rag_checkbox = Checkbox(\n",
    "    value=True,\n",
    "    description='Enable RAG (Retrieval-Augmented Generation)',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# Create buttons\n",
    "rag_generate_button = Button(description='Generate Response', button_style='primary')\n",
    "rag_clear_button = Button(description='Clear Output', button_style='warning')\n",
    "\n",
    "# Create an output area to display the result\n",
    "rag_output_area = Output()\n",
    "\n",
    "# Arrange the widgets\n",
    "rag_button_row = HBox([rag_generate_button, rag_clear_button])\n",
    "rag_ui = VBox([rag_input_box, rag_checkbox, rag_button_row, rag_output_area])\n",
    "display(rag_ui)\n",
    "\n",
    "def generate_rag_response(query: str, use_rag: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response using RAG or just the base model\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        use_rag: Whether to use RAG or just the base model\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if use_rag:\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = retrieve_relevant_documents(query, top_k=2)\n",
    "        \n",
    "        # Create context from retrieved documents\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc['content']}\" \n",
    "                              for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # Create the system message with context\n",
    "        system_message = f\"\"\"You are Bernd the Bread, a cynical and philosophical bread. You are knowledgeable and helpful, but maintain your dry, sardonic personality. Your answers are concise but informative.\n",
    "\n",
    "Use the following context to answer the user's question accurately:\n",
    "\n",
    "{context}\n",
    "\n",
    "Base your answer on the provided context, but feel free to add your own philosophical bread wisdom.\"\"\"\n",
    "    else:\n",
    "        # Use the original system message without RAG\n",
    "        system_message = \"You are Bernd the Bread. You are a cynical and philosophical bread. Your answers are short and concise.\"\n",
    "    \n",
    "    # Set up the messages for the chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    # Apply the model's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer([text], return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate model output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    # Remove the prompt tokens from the generated result\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response, relevant_docs if use_rag else None\n",
    "\n",
    "def rag_generate_response(_):\n",
    "    # Clear previous output\n",
    "    rag_output_area.clear_output()\n",
    "    \n",
    "    # Get the user prompt from the text area\n",
    "    query = rag_input_box.value\n",
    "    use_rag = rag_checkbox.value\n",
    "    \n",
    "    with rag_output_area:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"RAG Mode: {'Enabled' if use_rag else 'Disabled'}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if use_rag:\n",
    "            print(\"🔍 Retrieving relevant documents...\")\n",
    "            \n",
    "        try:\n",
    "            response, retrieved_docs = generate_rag_response(query, use_rag)\n",
    "            \n",
    "            if use_rag and retrieved_docs:\n",
    "                print(\"\\n📚 Retrieved Documents:\")\n",
    "                for i, doc in enumerate(retrieved_docs):\n",
    "                    print(f\"  {i+1}. {doc['title']} (Score: {doc['relevance_score']:.4f})\")\n",
    "                print()\n",
    "            \n",
    "            print(\"🍞 Bernd's Response:\")\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "def rag_clear_output(_):\n",
    "    rag_output_area.clear_output()\n",
    "\n",
    "# Link button events\n",
    "rag_generate_button.on_click(rag_generate_response)\n",
    "rag_clear_button.on_click(rag_clear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5fbe1",
   "metadata": {},
   "source": [
    "# Two Examples: Simple vs RAG-Enhanced\n",
    "\n",
    "This notebook demonstrates two approaches to using the language model:\n",
    "\n",
    "## 1. Original Simple Example (Cell 4)\n",
    "- Basic chat interface with Bernd the Bread\n",
    "- Uses only the base model without additional context\n",
    "- Simple prompt-response interaction\n",
    "- Good for general conversation and basic questions\n",
    "\n",
    "## 2. RAG-Enhanced Example (Cell 5)\n",
    "- Advanced interface with Retrieval-Augmented Generation\n",
    "- Can toggle RAG on/off to compare responses\n",
    "- Retrieves relevant documents from knowledge base\n",
    "- Provides more accurate and informed responses for specific topics\n",
    "\n",
    "---\n",
    "\n",
    "## RAG Demo - Example Queries\n",
    "\n",
    "Try these example queries in the **RAG-Enhanced Example** to see how RAG improves responses:\n",
    "\n",
    "### With RAG Enabled:\n",
    "1. **\"What is the difference between machine learning and deep learning?\"**\n",
    "   - The system will retrieve relevant documents about both topics and provide a comprehensive comparison.\n",
    "\n",
    "2. **\"How does RAG work?\"**\n",
    "   - Will find the specific document about RAG and explain it accurately.\n",
    "\n",
    "3. **\"What are neural networks?\"**\n",
    "   - Will retrieve the neural network document and provide detailed information.\n",
    "\n",
    "### With RAG Disabled:\n",
    "Try the same queries with RAG disabled to see how the base model responds without the additional context.\n",
    "\n",
    "### Compare with Simple Example:\n",
    "Try the same queries in the original simple example to see the difference between the base model alone vs. the RAG-enhanced version.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features of the RAG Implementation:\n",
    "\n",
    "- **No API Keys Required**: Uses local models and embeddings\n",
    "- **Lightweight**: Uses `all-MiniLM-L6-v2` for embeddings (80MB model)\n",
    "- **Fast Retrieval**: FAISS for efficient similarity search\n",
    "- **Customizable**: Easy to add your own documents to the knowledge base\n",
    "- **Binder Compatible**: All dependencies are available via pip\n",
    "- **Interactive**: Toggle RAG on/off to compare responses\n",
    "- **Side-by-side Comparison**: Compare simple vs. RAG-enhanced responses\n",
    "\n",
    "## How to Customize:\n",
    "\n",
    "1. **Add Your Own Documents**: Modify the `knowledge_base` list in the RAG setup cell\n",
    "2. **Adjust Retrieval**: Change `top_k` parameter in the retrieval function\n",
    "3. **Tune Generation**: Modify temperature and other generation parameters\n",
    "4. **Change Embedding Model**: Try different sentence-transformer models\n",
    "\n",
    "## Usage Instructions:\n",
    "\n",
    "1. **Run cells 1-3** to set up the model and RAG system\n",
    "2. **Use Cell 4** for simple interactions with Bernd the Bread\n",
    "3. **Use Cell 5** for RAG-enhanced interactions with document retrieval\n",
    "4. **Compare results** between the two approaches to understand the benefits of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35989a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
